{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
    "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
    "# Some part of the code was referenced from below                              #\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n",
    "        self.layer4 = self.make_layer(block, 128, layers[1])\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        print(nn.Sequential(*layers))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        feature = out.view(out.size(0), -1)\n",
    "        out = self.fc(feature)\n",
    "        return out, feature\n",
    "    \n",
    "model = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.7620\n",
      "Epoch [1/80], Step [200/500] Loss: 1.4864\n",
      "Epoch [1/80], Step [300/500] Loss: 1.3161\n",
      "Epoch [1/80], Step [400/500] Loss: 1.4461\n",
      "Epoch [1/80], Step [500/500] Loss: 1.1318\n",
      "Epoch [2/80], Step [100/500] Loss: 0.8508\n",
      "Epoch [2/80], Step [200/500] Loss: 0.9327\n",
      "Epoch [2/80], Step [300/500] Loss: 1.0433\n",
      "Epoch [2/80], Step [400/500] Loss: 0.8035\n",
      "Epoch [2/80], Step [500/500] Loss: 0.9743\n",
      "Epoch [3/80], Step [100/500] Loss: 0.7867\n",
      "Epoch [3/80], Step [200/500] Loss: 1.0103\n",
      "Epoch [3/80], Step [300/500] Loss: 0.7966\n",
      "Epoch [3/80], Step [400/500] Loss: 0.8904\n",
      "Epoch [3/80], Step [500/500] Loss: 0.5328\n",
      "Epoch [4/80], Step [100/500] Loss: 0.7963\n",
      "Epoch [4/80], Step [200/500] Loss: 0.8402\n",
      "Epoch [4/80], Step [300/500] Loss: 0.7290\n",
      "Epoch [4/80], Step [400/500] Loss: 0.4578\n",
      "Epoch [4/80], Step [500/500] Loss: 0.6119\n",
      "Epoch [5/80], Step [100/500] Loss: 0.5628\n",
      "Epoch [5/80], Step [200/500] Loss: 0.5955\n",
      "Epoch [5/80], Step [300/500] Loss: 0.7630\n",
      "Epoch [5/80], Step [400/500] Loss: 0.6859\n",
      "Epoch [5/80], Step [500/500] Loss: 0.5041\n",
      "Epoch [6/80], Step [100/500] Loss: 0.6099\n",
      "Epoch [6/80], Step [200/500] Loss: 0.5211\n",
      "Epoch [6/80], Step [300/500] Loss: 0.5898\n",
      "Epoch [6/80], Step [400/500] Loss: 0.6456\n",
      "Epoch [6/80], Step [500/500] Loss: 0.6779\n",
      "Epoch [7/80], Step [100/500] Loss: 0.4620\n",
      "Epoch [7/80], Step [200/500] Loss: 0.4802\n",
      "Epoch [7/80], Step [300/500] Loss: 0.5728\n",
      "Epoch [7/80], Step [400/500] Loss: 0.5456\n",
      "Epoch [7/80], Step [500/500] Loss: 0.5581\n",
      "Epoch [8/80], Step [100/500] Loss: 0.4469\n",
      "Epoch [8/80], Step [200/500] Loss: 0.6527\n",
      "Epoch [8/80], Step [300/500] Loss: 0.3872\n",
      "Epoch [8/80], Step [400/500] Loss: 0.4048\n",
      "Epoch [8/80], Step [500/500] Loss: 0.4807\n",
      "Epoch [9/80], Step [100/500] Loss: 0.4575\n",
      "Epoch [9/80], Step [200/500] Loss: 0.6074\n",
      "Epoch [9/80], Step [300/500] Loss: 0.4259\n",
      "Epoch [9/80], Step [400/500] Loss: 0.6794\n",
      "Epoch [9/80], Step [500/500] Loss: 0.4573\n",
      "Epoch [10/80], Step [100/500] Loss: 0.4087\n",
      "Epoch [10/80], Step [200/500] Loss: 0.5443\n",
      "Epoch [10/80], Step [300/500] Loss: 0.4903\n",
      "Epoch [10/80], Step [400/500] Loss: 0.5359\n",
      "Epoch [10/80], Step [500/500] Loss: 0.6661\n",
      "Epoch [11/80], Step [100/500] Loss: 0.4097\n",
      "Epoch [11/80], Step [200/500] Loss: 0.5394\n",
      "Epoch [11/80], Step [300/500] Loss: 0.3565\n",
      "Epoch [11/80], Step [400/500] Loss: 0.3973\n",
      "Epoch [11/80], Step [500/500] Loss: 0.4241\n",
      "Epoch [12/80], Step [100/500] Loss: 0.3622\n",
      "Epoch [12/80], Step [200/500] Loss: 0.4422\n",
      "Epoch [12/80], Step [300/500] Loss: 0.4456\n",
      "Epoch [12/80], Step [400/500] Loss: 0.5143\n",
      "Epoch [12/80], Step [500/500] Loss: 0.3588\n",
      "Epoch [13/80], Step [100/500] Loss: 0.5378\n",
      "Epoch [13/80], Step [200/500] Loss: 0.4828\n",
      "Epoch [13/80], Step [300/500] Loss: 0.3697\n",
      "Epoch [13/80], Step [400/500] Loss: 0.4957\n",
      "Epoch [13/80], Step [500/500] Loss: 0.3307\n",
      "Epoch [14/80], Step [100/500] Loss: 0.5120\n",
      "Epoch [14/80], Step [200/500] Loss: 0.3156\n",
      "Epoch [14/80], Step [300/500] Loss: 0.4722\n",
      "Epoch [14/80], Step [400/500] Loss: 0.4278\n",
      "Epoch [14/80], Step [500/500] Loss: 0.3556\n",
      "Epoch [15/80], Step [100/500] Loss: 0.3937\n",
      "Epoch [15/80], Step [200/500] Loss: 0.3252\n",
      "Epoch [15/80], Step [300/500] Loss: 0.3263\n",
      "Epoch [15/80], Step [400/500] Loss: 0.3808\n",
      "Epoch [15/80], Step [500/500] Loss: 0.3292\n",
      "Epoch [16/80], Step [100/500] Loss: 0.2905\n",
      "Epoch [16/80], Step [200/500] Loss: 0.2246\n",
      "Epoch [16/80], Step [300/500] Loss: 0.3694\n",
      "Epoch [16/80], Step [400/500] Loss: 0.3683\n",
      "Epoch [16/80], Step [500/500] Loss: 0.3085\n",
      "Epoch [17/80], Step [100/500] Loss: 0.2790\n",
      "Epoch [17/80], Step [200/500] Loss: 0.3811\n",
      "Epoch [17/80], Step [300/500] Loss: 0.3371\n",
      "Epoch [17/80], Step [400/500] Loss: 0.3646\n",
      "Epoch [17/80], Step [500/500] Loss: 0.3445\n",
      "Epoch [18/80], Step [100/500] Loss: 0.3009\n",
      "Epoch [18/80], Step [200/500] Loss: 0.2637\n",
      "Epoch [18/80], Step [300/500] Loss: 0.3223\n",
      "Epoch [18/80], Step [400/500] Loss: 0.2345\n",
      "Epoch [18/80], Step [500/500] Loss: 0.1657\n",
      "Epoch [19/80], Step [100/500] Loss: 0.3551\n",
      "Epoch [19/80], Step [200/500] Loss: 0.3121\n",
      "Epoch [19/80], Step [300/500] Loss: 0.3362\n",
      "Epoch [19/80], Step [400/500] Loss: 0.3219\n",
      "Epoch [19/80], Step [500/500] Loss: 0.2776\n",
      "Epoch [20/80], Step [100/500] Loss: 0.3086\n",
      "Epoch [20/80], Step [200/500] Loss: 0.2823\n",
      "Epoch [20/80], Step [300/500] Loss: 0.2665\n",
      "Epoch [20/80], Step [400/500] Loss: 0.1856\n",
      "Epoch [20/80], Step [500/500] Loss: 0.3222\n",
      "Epoch [21/80], Step [100/500] Loss: 0.2775\n",
      "Epoch [21/80], Step [200/500] Loss: 0.2039\n",
      "Epoch [21/80], Step [300/500] Loss: 0.2032\n",
      "Epoch [21/80], Step [400/500] Loss: 0.3260\n",
      "Epoch [21/80], Step [500/500] Loss: 0.1327\n",
      "Epoch [22/80], Step [100/500] Loss: 0.2199\n",
      "Epoch [22/80], Step [200/500] Loss: 0.1906\n",
      "Epoch [22/80], Step [300/500] Loss: 0.2652\n",
      "Epoch [22/80], Step [400/500] Loss: 0.1503\n",
      "Epoch [22/80], Step [500/500] Loss: 0.1833\n",
      "Epoch [23/80], Step [100/500] Loss: 0.2038\n",
      "Epoch [23/80], Step [200/500] Loss: 0.1753\n",
      "Epoch [23/80], Step [300/500] Loss: 0.1112\n",
      "Epoch [23/80], Step [400/500] Loss: 0.2052\n",
      "Epoch [23/80], Step [500/500] Loss: 0.1681\n",
      "Epoch [24/80], Step [100/500] Loss: 0.1552\n",
      "Epoch [24/80], Step [200/500] Loss: 0.2344\n",
      "Epoch [24/80], Step [300/500] Loss: 0.2464\n",
      "Epoch [24/80], Step [400/500] Loss: 0.1859\n",
      "Epoch [24/80], Step [500/500] Loss: 0.1641\n",
      "Epoch [25/80], Step [100/500] Loss: 0.1344\n",
      "Epoch [25/80], Step [200/500] Loss: 0.2045\n",
      "Epoch [25/80], Step [300/500] Loss: 0.2683\n",
      "Epoch [25/80], Step [400/500] Loss: 0.1358\n",
      "Epoch [25/80], Step [500/500] Loss: 0.1510\n",
      "Epoch [26/80], Step [100/500] Loss: 0.0987\n",
      "Epoch [26/80], Step [200/500] Loss: 0.1185\n",
      "Epoch [26/80], Step [300/500] Loss: 0.2421\n",
      "Epoch [26/80], Step [400/500] Loss: 0.2314\n",
      "Epoch [26/80], Step [500/500] Loss: 0.3034\n",
      "Epoch [27/80], Step [100/500] Loss: 0.1658\n",
      "Epoch [27/80], Step [200/500] Loss: 0.1429\n",
      "Epoch [27/80], Step [300/500] Loss: 0.0950\n",
      "Epoch [27/80], Step [400/500] Loss: 0.2183\n",
      "Epoch [27/80], Step [500/500] Loss: 0.1971\n",
      "Epoch [28/80], Step [100/500] Loss: 0.1468\n",
      "Epoch [28/80], Step [200/500] Loss: 0.2006\n",
      "Epoch [28/80], Step [300/500] Loss: 0.0903\n",
      "Epoch [28/80], Step [400/500] Loss: 0.2042\n",
      "Epoch [28/80], Step [500/500] Loss: 0.1100\n",
      "Epoch [29/80], Step [100/500] Loss: 0.1973\n",
      "Epoch [29/80], Step [200/500] Loss: 0.1463\n",
      "Epoch [29/80], Step [300/500] Loss: 0.2270\n",
      "Epoch [29/80], Step [400/500] Loss: 0.2158\n",
      "Epoch [29/80], Step [500/500] Loss: 0.1092\n",
      "Epoch [30/80], Step [100/500] Loss: 0.1034\n",
      "Epoch [30/80], Step [200/500] Loss: 0.1364\n",
      "Epoch [30/80], Step [300/500] Loss: 0.1951\n",
      "Epoch [30/80], Step [400/500] Loss: 0.1521\n",
      "Epoch [30/80], Step [500/500] Loss: 0.2201\n",
      "Epoch [31/80], Step [100/500] Loss: 0.1062\n",
      "Epoch [31/80], Step [200/500] Loss: 0.1394\n",
      "Epoch [31/80], Step [300/500] Loss: 0.1046\n",
      "Epoch [31/80], Step [400/500] Loss: 0.1885\n",
      "Epoch [31/80], Step [500/500] Loss: 0.0996\n",
      "Epoch [32/80], Step [100/500] Loss: 0.1300\n",
      "Epoch [32/80], Step [200/500] Loss: 0.0890\n",
      "Epoch [32/80], Step [300/500] Loss: 0.2239\n",
      "Epoch [32/80], Step [400/500] Loss: 0.1145\n",
      "Epoch [32/80], Step [500/500] Loss: 0.1568\n",
      "Epoch [33/80], Step [100/500] Loss: 0.2150\n",
      "Epoch [33/80], Step [200/500] Loss: 0.2503\n",
      "Epoch [33/80], Step [300/500] Loss: 0.1269\n",
      "Epoch [33/80], Step [400/500] Loss: 0.2005\n",
      "Epoch [33/80], Step [500/500] Loss: 0.0640\n",
      "Epoch [34/80], Step [100/500] Loss: 0.2508\n",
      "Epoch [34/80], Step [200/500] Loss: 0.1353\n",
      "Epoch [34/80], Step [300/500] Loss: 0.1320\n",
      "Epoch [34/80], Step [400/500] Loss: 0.2576\n",
      "Epoch [34/80], Step [500/500] Loss: 0.1736\n",
      "Epoch [35/80], Step [100/500] Loss: 0.0920\n",
      "Epoch [35/80], Step [200/500] Loss: 0.1588\n",
      "Epoch [35/80], Step [300/500] Loss: 0.1702\n",
      "Epoch [35/80], Step [400/500] Loss: 0.1352\n",
      "Epoch [35/80], Step [500/500] Loss: 0.2595\n",
      "Epoch [36/80], Step [100/500] Loss: 0.1216\n",
      "Epoch [36/80], Step [200/500] Loss: 0.0600\n",
      "Epoch [36/80], Step [300/500] Loss: 0.2238\n",
      "Epoch [36/80], Step [400/500] Loss: 0.1577\n",
      "Epoch [36/80], Step [500/500] Loss: 0.0819\n",
      "Epoch [37/80], Step [100/500] Loss: 0.0492\n",
      "Epoch [37/80], Step [200/500] Loss: 0.1880\n",
      "Epoch [37/80], Step [300/500] Loss: 0.1334\n",
      "Epoch [37/80], Step [400/500] Loss: 0.0821\n",
      "Epoch [37/80], Step [500/500] Loss: 0.1299\n",
      "Epoch [38/80], Step [100/500] Loss: 0.1997\n",
      "Epoch [38/80], Step [200/500] Loss: 0.1543\n",
      "Epoch [38/80], Step [300/500] Loss: 0.1025\n",
      "Epoch [38/80], Step [400/500] Loss: 0.1469\n",
      "Epoch [38/80], Step [500/500] Loss: 0.1086\n",
      "Epoch [39/80], Step [100/500] Loss: 0.0598\n",
      "Epoch [39/80], Step [200/500] Loss: 0.1370\n",
      "Epoch [39/80], Step [300/500] Loss: 0.0796\n",
      "Epoch [39/80], Step [400/500] Loss: 0.0965\n",
      "Epoch [39/80], Step [500/500] Loss: 0.0565\n",
      "Epoch [40/80], Step [100/500] Loss: 0.1042\n",
      "Epoch [40/80], Step [200/500] Loss: 0.0951\n",
      "Epoch [40/80], Step [300/500] Loss: 0.1390\n",
      "Epoch [40/80], Step [400/500] Loss: 0.1285\n",
      "Epoch [40/80], Step [500/500] Loss: 0.1095\n",
      "Epoch [41/80], Step [100/500] Loss: 0.0991\n",
      "Epoch [41/80], Step [200/500] Loss: 0.1330\n",
      "Epoch [41/80], Step [300/500] Loss: 0.1365\n",
      "Epoch [41/80], Step [400/500] Loss: 0.0722\n",
      "Epoch [41/80], Step [500/500] Loss: 0.0953\n",
      "Epoch [42/80], Step [100/500] Loss: 0.0893\n",
      "Epoch [42/80], Step [200/500] Loss: 0.0957\n",
      "Epoch [42/80], Step [300/500] Loss: 0.1078\n",
      "Epoch [42/80], Step [400/500] Loss: 0.1079\n",
      "Epoch [42/80], Step [500/500] Loss: 0.0997\n",
      "Epoch [43/80], Step [100/500] Loss: 0.0571\n",
      "Epoch [43/80], Step [200/500] Loss: 0.1203\n",
      "Epoch [43/80], Step [300/500] Loss: 0.0823\n",
      "Epoch [43/80], Step [400/500] Loss: 0.1440\n",
      "Epoch [43/80], Step [500/500] Loss: 0.1328\n",
      "Epoch [44/80], Step [100/500] Loss: 0.1056\n",
      "Epoch [44/80], Step [200/500] Loss: 0.1698\n",
      "Epoch [44/80], Step [300/500] Loss: 0.0796\n",
      "Epoch [44/80], Step [400/500] Loss: 0.1056\n",
      "Epoch [44/80], Step [500/500] Loss: 0.0566\n",
      "Epoch [45/80], Step [100/500] Loss: 0.0647\n",
      "Epoch [45/80], Step [200/500] Loss: 0.0939\n",
      "Epoch [45/80], Step [300/500] Loss: 0.1777\n",
      "Epoch [45/80], Step [400/500] Loss: 0.1678\n",
      "Epoch [45/80], Step [500/500] Loss: 0.0786\n",
      "Epoch [46/80], Step [100/500] Loss: 0.0352\n",
      "Epoch [46/80], Step [200/500] Loss: 0.0707\n",
      "Epoch [46/80], Step [300/500] Loss: 0.1189\n",
      "Epoch [46/80], Step [400/500] Loss: 0.0660\n",
      "Epoch [46/80], Step [500/500] Loss: 0.0420\n",
      "Epoch [47/80], Step [100/500] Loss: 0.0795\n",
      "Epoch [47/80], Step [200/500] Loss: 0.0730\n",
      "Epoch [47/80], Step [300/500] Loss: 0.1127\n",
      "Epoch [47/80], Step [400/500] Loss: 0.1030\n",
      "Epoch [47/80], Step [500/500] Loss: 0.0612\n",
      "Epoch [48/80], Step [100/500] Loss: 0.1484\n",
      "Epoch [48/80], Step [200/500] Loss: 0.0835\n",
      "Epoch [48/80], Step [300/500] Loss: 0.0193\n",
      "Epoch [48/80], Step [400/500] Loss: 0.1687\n",
      "Epoch [48/80], Step [500/500] Loss: 0.1245\n",
      "Epoch [49/80], Step [100/500] Loss: 0.1610\n",
      "Epoch [49/80], Step [200/500] Loss: 0.0833\n",
      "Epoch [49/80], Step [300/500] Loss: 0.0564\n",
      "Epoch [49/80], Step [400/500] Loss: 0.0518\n",
      "Epoch [49/80], Step [500/500] Loss: 0.0771\n",
      "Epoch [50/80], Step [100/500] Loss: 0.0823\n",
      "Epoch [50/80], Step [200/500] Loss: 0.0698\n",
      "Epoch [50/80], Step [300/500] Loss: 0.0445\n",
      "Epoch [50/80], Step [400/500] Loss: 0.1532\n",
      "Epoch [50/80], Step [500/500] Loss: 0.0795\n",
      "Epoch [51/80], Step [100/500] Loss: 0.0595\n",
      "Epoch [51/80], Step [200/500] Loss: 0.0487\n",
      "Epoch [51/80], Step [300/500] Loss: 0.0580\n",
      "Epoch [51/80], Step [400/500] Loss: 0.0956\n",
      "Epoch [51/80], Step [500/500] Loss: 0.1040\n",
      "Epoch [52/80], Step [100/500] Loss: 0.0677\n",
      "Epoch [52/80], Step [200/500] Loss: 0.1027\n",
      "Epoch [52/80], Step [300/500] Loss: 0.0609\n",
      "Epoch [52/80], Step [400/500] Loss: 0.1002\n",
      "Epoch [52/80], Step [500/500] Loss: 0.0278\n",
      "Epoch [53/80], Step [100/500] Loss: 0.1003\n",
      "Epoch [53/80], Step [200/500] Loss: 0.0526\n",
      "Epoch [53/80], Step [300/500] Loss: 0.0655\n",
      "Epoch [53/80], Step [400/500] Loss: 0.0869\n",
      "Epoch [53/80], Step [500/500] Loss: 0.0610\n",
      "Epoch [54/80], Step [100/500] Loss: 0.0810\n",
      "Epoch [54/80], Step [200/500] Loss: 0.0550\n",
      "Epoch [54/80], Step [300/500] Loss: 0.0941\n",
      "Epoch [54/80], Step [400/500] Loss: 0.0836\n",
      "Epoch [54/80], Step [500/500] Loss: 0.0481\n",
      "Epoch [55/80], Step [100/500] Loss: 0.1402\n",
      "Epoch [55/80], Step [200/500] Loss: 0.1775\n",
      "Epoch [55/80], Step [300/500] Loss: 0.0688\n",
      "Epoch [55/80], Step [400/500] Loss: 0.0677\n",
      "Epoch [55/80], Step [500/500] Loss: 0.0529\n",
      "Epoch [56/80], Step [100/500] Loss: 0.1243\n",
      "Epoch [56/80], Step [200/500] Loss: 0.1287\n",
      "Epoch [56/80], Step [300/500] Loss: 0.1005\n",
      "Epoch [56/80], Step [400/500] Loss: 0.0484\n",
      "Epoch [56/80], Step [500/500] Loss: 0.0538\n",
      "Epoch [57/80], Step [100/500] Loss: 0.0576\n",
      "Epoch [57/80], Step [200/500] Loss: 0.0377\n",
      "Epoch [57/80], Step [300/500] Loss: 0.0804\n",
      "Epoch [57/80], Step [400/500] Loss: 0.0359\n",
      "Epoch [57/80], Step [500/500] Loss: 0.0491\n",
      "Epoch [58/80], Step [100/500] Loss: 0.0973\n",
      "Epoch [58/80], Step [200/500] Loss: 0.0453\n",
      "Epoch [58/80], Step [300/500] Loss: 0.0851\n",
      "Epoch [58/80], Step [400/500] Loss: 0.0530\n",
      "Epoch [58/80], Step [500/500] Loss: 0.0807\n",
      "Epoch [59/80], Step [100/500] Loss: 0.1032\n",
      "Epoch [59/80], Step [200/500] Loss: 0.0829\n",
      "Epoch [59/80], Step [300/500] Loss: 0.1001\n",
      "Epoch [59/80], Step [400/500] Loss: 0.0534\n",
      "Epoch [59/80], Step [500/500] Loss: 0.0473\n",
      "Epoch [60/80], Step [100/500] Loss: 0.0944\n",
      "Epoch [60/80], Step [200/500] Loss: 0.0500\n",
      "Epoch [60/80], Step [300/500] Loss: 0.0287\n",
      "Epoch [60/80], Step [400/500] Loss: 0.0756\n",
      "Epoch [60/80], Step [500/500] Loss: 0.0856\n",
      "Epoch [61/80], Step [100/500] Loss: 0.0769\n",
      "Epoch [61/80], Step [200/500] Loss: 0.1055\n",
      "Epoch [61/80], Step [300/500] Loss: 0.1871\n",
      "Epoch [61/80], Step [400/500] Loss: 0.0594\n",
      "Epoch [61/80], Step [500/500] Loss: 0.1135\n",
      "Epoch [62/80], Step [100/500] Loss: 0.0735\n",
      "Epoch [62/80], Step [200/500] Loss: 0.0490\n",
      "Epoch [62/80], Step [300/500] Loss: 0.0707\n",
      "Epoch [62/80], Step [400/500] Loss: 0.0405\n",
      "Epoch [62/80], Step [500/500] Loss: 0.0264\n",
      "Epoch [63/80], Step [100/500] Loss: 0.0631\n",
      "Epoch [63/80], Step [200/500] Loss: 0.1074\n",
      "Epoch [63/80], Step [300/500] Loss: 0.0441\n",
      "Epoch [63/80], Step [400/500] Loss: 0.0885\n",
      "Epoch [63/80], Step [500/500] Loss: 0.0352\n",
      "Epoch [64/80], Step [100/500] Loss: 0.0347\n",
      "Epoch [64/80], Step [200/500] Loss: 0.0199\n",
      "Epoch [64/80], Step [300/500] Loss: 0.0540\n",
      "Epoch [64/80], Step [400/500] Loss: 0.0168\n",
      "Epoch [64/80], Step [500/500] Loss: 0.0414\n",
      "Epoch [65/80], Step [100/500] Loss: 0.0476\n",
      "Epoch [65/80], Step [200/500] Loss: 0.0452\n",
      "Epoch [65/80], Step [300/500] Loss: 0.0559\n",
      "Epoch [65/80], Step [400/500] Loss: 0.0293\n",
      "Epoch [65/80], Step [500/500] Loss: 0.0487\n",
      "Epoch [66/80], Step [100/500] Loss: 0.0416\n",
      "Epoch [66/80], Step [200/500] Loss: 0.1718\n",
      "Epoch [66/80], Step [300/500] Loss: 0.1649\n",
      "Epoch [66/80], Step [400/500] Loss: 0.0990\n",
      "Epoch [66/80], Step [500/500] Loss: 0.1420\n",
      "Epoch [67/80], Step [100/500] Loss: 0.0498\n",
      "Epoch [67/80], Step [200/500] Loss: 0.0816\n",
      "Epoch [67/80], Step [300/500] Loss: 0.0521\n",
      "Epoch [67/80], Step [400/500] Loss: 0.1197\n",
      "Epoch [67/80], Step [500/500] Loss: 0.0223\n",
      "Epoch [68/80], Step [100/500] Loss: 0.0131\n",
      "Epoch [68/80], Step [200/500] Loss: 0.0748\n",
      "Epoch [68/80], Step [300/500] Loss: 0.0296\n",
      "Epoch [68/80], Step [400/500] Loss: 0.1545\n",
      "Epoch [68/80], Step [500/500] Loss: 0.0389\n",
      "Epoch [69/80], Step [100/500] Loss: 0.0275\n",
      "Epoch [69/80], Step [200/500] Loss: 0.0610\n",
      "Epoch [69/80], Step [300/500] Loss: 0.0417\n",
      "Epoch [69/80], Step [400/500] Loss: 0.0705\n",
      "Epoch [69/80], Step [500/500] Loss: 0.0636\n",
      "Epoch [70/80], Step [100/500] Loss: 0.0645\n",
      "Epoch [70/80], Step [200/500] Loss: 0.0524\n",
      "Epoch [70/80], Step [300/500] Loss: 0.0583\n",
      "Epoch [70/80], Step [400/500] Loss: 0.0695\n",
      "Epoch [70/80], Step [500/500] Loss: 0.1087\n",
      "Epoch [71/80], Step [100/500] Loss: 0.0754\n",
      "Epoch [71/80], Step [200/500] Loss: 0.0501\n",
      "Epoch [71/80], Step [300/500] Loss: 0.1551\n",
      "Epoch [71/80], Step [400/500] Loss: 0.0902\n",
      "Epoch [71/80], Step [500/500] Loss: 0.0577\n",
      "Epoch [72/80], Step [100/500] Loss: 0.0819\n",
      "Epoch [72/80], Step [200/500] Loss: 0.0426\n",
      "Epoch [72/80], Step [300/500] Loss: 0.0750\n",
      "Epoch [72/80], Step [400/500] Loss: 0.0539\n",
      "Epoch [72/80], Step [500/500] Loss: 0.0350\n",
      "Epoch [73/80], Step [100/500] Loss: 0.0463\n",
      "Epoch [73/80], Step [200/500] Loss: 0.0163\n",
      "Epoch [73/80], Step [300/500] Loss: 0.0157\n",
      "Epoch [73/80], Step [400/500] Loss: 0.0308\n",
      "Epoch [73/80], Step [500/500] Loss: 0.0866\n",
      "Epoch [74/80], Step [100/500] Loss: 0.1489\n",
      "Epoch [74/80], Step [200/500] Loss: 0.0564\n",
      "Epoch [74/80], Step [300/500] Loss: 0.0590\n",
      "Epoch [74/80], Step [400/500] Loss: 0.0256\n",
      "Epoch [74/80], Step [500/500] Loss: 0.0384\n",
      "Epoch [75/80], Step [100/500] Loss: 0.0282\n",
      "Epoch [75/80], Step [200/500] Loss: 0.0237\n",
      "Epoch [75/80], Step [300/500] Loss: 0.0339\n",
      "Epoch [75/80], Step [400/500] Loss: 0.0417\n",
      "Epoch [75/80], Step [500/500] Loss: 0.0239\n",
      "Epoch [76/80], Step [100/500] Loss: 0.0394\n",
      "Epoch [76/80], Step [200/500] Loss: 0.0469\n",
      "Epoch [76/80], Step [300/500] Loss: 0.0238\n",
      "Epoch [76/80], Step [400/500] Loss: 0.0476\n",
      "Epoch [76/80], Step [500/500] Loss: 0.0997\n",
      "Epoch [77/80], Step [100/500] Loss: 0.0319\n",
      "Epoch [77/80], Step [200/500] Loss: 0.0346\n",
      "Epoch [77/80], Step [300/500] Loss: 0.1564\n",
      "Epoch [77/80], Step [400/500] Loss: 0.0476\n",
      "Epoch [77/80], Step [500/500] Loss: 0.0636\n",
      "Epoch [78/80], Step [100/500] Loss: 0.0534\n",
      "Epoch [78/80], Step [200/500] Loss: 0.0281\n",
      "Epoch [78/80], Step [300/500] Loss: 0.0860\n",
      "Epoch [78/80], Step [400/500] Loss: 0.0456\n",
      "Epoch [78/80], Step [500/500] Loss: 0.0632\n",
      "Epoch [79/80], Step [100/500] Loss: 0.0211\n",
      "Epoch [79/80], Step [200/500] Loss: 0.0344\n",
      "Epoch [79/80], Step [300/500] Loss: 0.0829\n",
      "Epoch [79/80], Step [400/500] Loss: 0.0173\n",
      "Epoch [79/80], Step [500/500] Loss: 0.0470\n",
      "Epoch [80/80], Step [100/500] Loss: 0.0201\n",
      "Epoch [80/80], Step [200/500] Loss: 0.0661\n",
      "Epoch [80/80], Step [300/500] Loss: 0.0192\n",
      "Epoch [80/80], Step [400/500] Loss: 0.0663\n",
      "Epoch [80/80], Step [500/500] Loss: 0.0249\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images)\n",
    "        # print(labels)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs,feature = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet_128.ckpt')\n",
    "torch.save(model, 'model_128.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('resnet.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[0.13603687 0.12749322 0.78440607 ... 1.46879411 1.68511033 1.52412641]\n",
      "[3. 8. 8. ... 5. 1. 7.]\n",
      "Accuracy of the model on the test images: 89 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model  \n",
    "pred=[]\n",
    "total_feature=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i=0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,feature = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # print(feature.cpu().detach().numpy().shape)\n",
    "        pred=np.concatenate((pred,predicted.cpu().numpy()))\n",
    "        total_feature=np.append(total_feature,feature.cpu().detach().numpy())\n",
    "    print(total)\n",
    "    print(total_feature)\n",
    "    print(pred)\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet1.ckpt')\n",
    "torch.save(model, 'model1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.reshape(total_feature,(10000,-1)).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61 62 64 65 62 61 63 62 61 62 63 62 63 65 66 66 65 63 62 62 61 58 58 58\n",
      " 58 56 54 55 56 53 49 45 59 59 62 65 63 62 64 63 63 61 60 62 63 66 67 67\n",
      " 66 62 60 59 56 54 54 55 58 57 56 55 56 53 49 46 59 59 61 65 62 63 64 64\n",
      " 63 63 61 61 63 65 65 66 66 62 56 47 43 38 39 44 47 52 56 54 55 54 50 47\n",
      " 60 60 62 68]\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#test_d = torchvision.datasets.CIFAR10(root='./data/',train=False)\n",
    "test_l = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=10000)\n",
    "for images, labels in test_l:\n",
    "    a=np.reshape(images.numpy()*100,(10000,-1)).astype('int')\n",
    "    print(a[0][:100])\n",
    "    print(labels.cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64)\n",
      "[ 3.  8.  8. 10.  6.  6.  1.  6.  3.  1. 10.  9.  7.  7.  9.  8.  5.  7.\n",
      "  8.  6.  3. 10.  4.  9.  5.  2.  3. 10.  9.  6.  6.  5.  4.  5.  9.  2.\n",
      "  4.  1.  9.  5.  4.  6.  3.  6. 10.  9.  3.  9.  7.  2.  9.  8. 10.  3.\n",
      "  8.  8.  7.  5.  7.  5.  7.  5.  6.  5.  6.  2.  1.  2.  3.  7.  2.  6.\n",
      "  8.  8. 10.  2.  9.  3.  5.  8.  8.  9.  1.  7.  0.  5.  2.  3.  8.  9.\n",
      " 10.  3.  8.  6.  4.  3.  6. 10. 10.  7.  4.  5.  6.  3.  1.  1.  3.  6.\n",
      "  8.  7.  4. 10.  2.  2.  1.  3. 10.  4.  6.  7.  8.  3.  1.  2.  8. 10.\n",
      "  8.  3.  3.  2.  4.  1.  8.  9.  1.  2.  9.  7.  2.  8.  6.  5.  6.  3.\n",
      "  8.  7.  6.  5.  5.  2.  8.  9.  6. 10. 10.  5.  2.  9.  5.  4.  2.  1.\n",
      "  6.  6.  8.  4.  8.  4.  5. 10.  9.  6.  9.  8.  9.  9.  3.  7.  3. 10.\n",
      " 10.  5.  2.  2.  3.  8.  6.  3.  2. 10.  5.  8. 10.  1.  7.  2.  8.  8.\n",
      "  7.  8.  5.  1.  8.  0.  1.  3. 10.  5.  7.  9.  7.  4.  5.  9.  8. 10.\n",
      "  7.  9.  8.  2.  7.  5.  9.  3.  3.  9.  9.  4.  5.  6.  5.  1.  5.  8.\n",
      "  8. 10.  4. 10.  5.  5.  1.  1.  8.  9. 10.  3.  1.  8.  2.  2.  5.  3.\n",
      "  9.  9.  4. 10.  3. 10. 10.  9.  8.  1.  5.  7. 10.  8.  2.  4.  7. 10.\n",
      "  2.  3.  6.  5.  8.  6. 10.  3.  4.  3.  9. 10.  6.  1. 10.  9.  1. 10.\n",
      "  7.  9.  1.  2.  6.  9.  3.  4.  6. 10. 10.  6.  6.  6.  3.  2.  6.  1.\n",
      "  8.  2.  1.  2.  8.  6.  0. 10.  4. 10.  7.  7.  5.  5.  3.  5.  2.  3.\n",
      "  4.  1.  7.  5.  4.  6.  1.  9.  3.  6.  6.  9.  3.  8. 10.  7.  2.  6.\n",
      "  2.  5.  8.  5.  4.  6.  8.  9.  9.  1. 10.  2.  2.  4.  5.  2.  0. 10.\n",
      "  9.  5.  8.  1.  9.  4.  1.  3.  8.  1.  2.  7.  9.  4.  2.  7. 10.  7.\n",
      " 10.  6.  6.  9. 10.  0.  5.  8.  7.  2.  2.  5.  1.  2.  6.  2.  9.  6.\n",
      "  3.  3. 10.  3.  9.  1.  7.  8.  8.  4. 10.  1.  8.  2.  7.  1.  5.  6.\n",
      "  1.  9. 10.  7.  2.  7.  4.  4. 10. 10.  2.  9.  6.  4. 10.  6.  2.  5.\n",
      "  3.  0.  9.  7.  2.  5.  3.  1.  1.  4.  9.  9.  5.  7.  5. 10.  2.  2.\n",
      "  2.  9.  7.  3.  9.  4.  4.  5.  4.  0.  5.  6.  1.  4.  3.  4.  4.  3.\n",
      "  7.  8.  3.  7.  8. 10.  3.  7.  5. 10.  5.  4.  8.  6.  8.  5.  5.  0.\n",
      "  9.  9.  4. 10.  1. 10.  8.  1.  1.  8. 10.  2.  2. 10.  4.  6.  5.  4.\n",
      "  9.  4.  7.  9.  0.  4.  5.  6.  6.  1.  5.  3.  8.  9.  7.  8.  5.  7.\n",
      " 10.  7. 10.  5. 10. 10.  4.  6.  9. 10.  9.  5.  6.  6.  6.  2.  9. 10.\n",
      "  1.  7.  6.  7.  5.  9.  1.  6.  2.  5.  5.  7.  8.  5.  9.  4.  6.  4.\n",
      "  3.  2. 10.  7.  6.  2.  2.  3.  9.  7.  9.  2.  6.  7.  1.  3.  6.  6.\n",
      "  8.  8.  7.  5.  4. 10.  8.  4. 10.  9.  3.  4.  8.  9.  6.  9.  2.  6.\n",
      "  1.  4.  7.  3.  3.  3.  8.  5. 10.  2.  1.  6.  4.  3.  3.  9.  6.  9.\n",
      "  8.  8.  5.  8.  6.  6.  2.  1.  7.  7.  1.  9.  7.  9.  9.  4.  4.  1.\n",
      "  2.  5.  6.  8.  7.  6.  8.  3. 10.  5.  5.  3. 10.  7.  9.  1.  3.  4.\n",
      "  4.  5.  3.  9.  5.  6.  9.  2.  1.  1.  4.  1.  9.  7.  7.  6.  3.  0.\n",
      "  9. 10.  1.  3.  6.  3.  6.  3.  2. 10.  3.  9. 10.  5.  1.  6.  4.  0.\n",
      "  9.  2.  9.  6.  3. 10.  3.  2.  0.  7.  8.  3.  8.  2.  7.  5.  7.  2.\n",
      "  4.  8.  7.  4.  2.  9.  8.  8.  6.  0.  8.  7.  4.  3.  3.  8.  4.  9.\n",
      "  4.  8.  8.  1.  8.  2.  1.  2.  6.  5.  5.  0.  7.  9.  1.  4.  1.  4.\n",
      "  1.  5.  2.  7. 10.  7.  2.  7.  4.  6.  2.  5.  9.  2.  9.  1.  2.  2.\n",
      "  6.  8.  2.  1.  3.  6.  6. 10.  1.  2.  7. 10.  5.  4.  0.  9.  6.  4.\n",
      " 10.  2.  2.  6. 10.  5.  9.  1.  7.  6.  7. 10.  3.  9.  6.  8.  4. 10.\n",
      "  5.  4.  7.  7.  4.  4.  7.  2.  7.  1.  2.  7.  4.  4.  8.  4.  7.  7.\n",
      "  2.  5.  7.  2. 10.  8.  9.  5.  8.  3.  6.  2. 10.  8.  7.  3.  7.  6.\n",
      "  5.  3.  1.  3.  2.  2.  5.  5.  1.  2.  9.  2.  7. 10.  7.  2.  1.  2.\n",
      "  2. 10.  2.  4.  7.  9.  0.  1. 10.  7.  7. 10.  7.  8.  4.  6.  3.  3.\n",
      " 10.  1.  3.  7. 10.  1.  3.  1.  4.  2.  3.  8.  4.  2.  3.  7.  8.  4.\n",
      "  2. 10.  9. 10. 10.  1. 10.  4.  4.  6.  7.  3.  1.  1.  3.  7.  7.  5.\n",
      "  2.  6.  6.  5.  8.  7.  1.  6.  8.  8.  3.  3. 10.  4. 10.  1.  3.  8.\n",
      "  8. 10.  6.  9.  9.  4.  7.  3.  8.  6. 10. 10.  4.  2.  3.  2.  7.  2.\n",
      "  2.  5.  9.  8.  9.  1.  7.  4. 10.  7. 10.  1.  4.  8.  3.  9.  6.  8.\n",
      "  4.  7. 10.  3.  7.  8.  9.  1.  1.  6.  6.  6.  3.  9.  1.  9.  9.  4.\n",
      "  6.  1.  7. 10.  6.  8.  8.  9.  2.  9. 10.  4.  7.  8.  3.  1.  2. 10.\n",
      "  1.  5.  8.  7.  4.  5.  8.  1.  3.  8.]\n"
     ]
    }
   ],
   "source": [
    "b=a[:1000]\n",
    "p=pred[:1000]\n",
    "print(b.shape)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  8.  8. 10.  6.  6.  1.  6.  3.  1. 10.  9.  7.  7.  9.  8.  5.  7.\n",
      "  8.  6.  3. 10.  4.  9.  5.  2.  3. 10.  9.  6.  6.  5.  4.  5.  9.  2.\n",
      "  4.  1.  9.  5.  4.  6.  3.  6. 10.  9.  3.  9.  7.  2.  9.  8. 10.  3.\n",
      "  8.  8.  7.  5.  7.  5.  7.  5.  6.  5.  6.  2.  1.  2.  3.  7.  2.  6.\n",
      "  8.  8. 10.  2.  9.  3.  5.  8.  8.  9.  1.  7.  0.  5.  2.  3.  8.  9.\n",
      " 10.  3.  8.  6.  4.  3.  6. 10. 10.  7.  4.  5.  6.  3.  1.  1.  3.  6.\n",
      "  8.  7.  4. 10.  2.  2.  1.  3. 10.  4.  6.  7.  8.  3.  1.  2.  8. 10.\n",
      "  8.  3.  3.  2.  4.  1.  8.  9.  1.  2.  9.  7.  2.  8.  6.  5.  6.  3.\n",
      "  8.  7.  6.  5.  5.  2.  8.  9.  6. 10. 10.  5.  2.  9.  5.  4.  2.  1.\n",
      "  6.  6.  8.  4.  8.  4.  5. 10.  9.  6.  9.  8.  9.  9.  3.  7.  3. 10.\n",
      " 10.  5.  2.  2.  3.  8.  6.  3.  2. 10.  5.  8. 10.  1.  7.  2.  8.  8.\n",
      "  7.  8.  5.  1.  8.  0.  1.  3. 10.  5.  7.  9.  7.  4.  5.  9.  8. 10.\n",
      "  7.  9.  8.  2.  7.  5.  9.  3.  3.  9.  9.  4.  5.  6.  5.  1.  5.  8.\n",
      "  8. 10.  4. 10.  5.  5.  1.  1.  8.  9. 10.  3.  1.  8.  2.  2.  5.  3.\n",
      "  9.  9.  4. 10.  3. 10. 10.  9.  8.  1.  5.  7. 10.  8.  2.  4.  7. 10.\n",
      "  2.  3.  6.  5.  8.  6. 10.  3.  4.  3.  9. 10.  6.  1. 10.  9.  1. 10.\n",
      "  7.  9.  1.  2.  6.  9.  3.  4.  6. 10. 10.  6.  6.  6.  3.  2.  6.  1.\n",
      "  8.  2.  1.  2.  8.  6.  0. 10.  4. 10.  7.  7.  5.  5.  3.  5.  2.  3.\n",
      "  4.  1.  7.  5.  4.  6.  1.  9.  3.  6.  6.  9.  3.  8. 10.  7.  2.  6.\n",
      "  2.  5.  8.  5.  4.  6.  8.  9.  9.  1. 10.  2.  2.  4.  5.  2.  0. 10.\n",
      "  9.  5.  8.  1.  9.  4.  1.  3.  8.  1.  2.  7.  9.  4.  2.  7. 10.  7.\n",
      " 10.  6.  6.  9. 10.  0.  5.  8.  7.  2.  2.  5.  1.  2.  6.  2.  9.  6.\n",
      "  3.  3. 10.  3.  9.  1.  7.  8.  8.  4. 10.  1.  8.  2.  7.  1.  5.  6.\n",
      "  1.  9. 10.  7.  2.  7.  4.  4. 10. 10.  2.  9.  6.  4. 10.  6.  2.  5.\n",
      "  3.  0.  9.  7.  2.  5.  3.  1.  1.  4.  9.  9.  5.  7.  5. 10.  2.  2.\n",
      "  2.  9.  7.  3.  9.  4.  4.  5.  4.  0.  5.  6.  1.  4.  3.  4.  4.  3.\n",
      "  7.  8.  3.  7.  8. 10.  3.  7.  5. 10.  5.  4.  8.  6.  8.  5.  5.  0.\n",
      "  9.  9.  4. 10.  1. 10.  8.  1.  1.  8. 10.  2.  2. 10.  4.  6.  5.  4.\n",
      "  9.  4.  7.  9.  0.  4.  5.  6.  6.  1.  5.  3.  8.  9.  7.  8.  5.  7.\n",
      " 10.  7. 10.  5. 10. 10.  4.  6.  9. 10.  9.  5.  6.  6.  6.  2.  9. 10.\n",
      "  1.  7.  6.  7.  5.  9.  1.  6.  2.  5.  5.  7.  8.  5.  9.  4.  6.  4.\n",
      "  3.  2. 10.  7.  6.  2.  2.  3.  9.  7.  9.  2.  6.  7.  1.  3.  6.  6.\n",
      "  8.  8.  7.  5.  4. 10.  8.  4. 10.  9.  3.  4.  8.  9.  6.  9.  2.  6.\n",
      "  1.  4.  7.  3.  3.  3.  8.  5. 10.  2.  1.  6.  4.  3.  3.  9.  6.  9.\n",
      "  8.  8.  5.  8.  6.  6.  2.  1.  7.  7.  1.  9.  7.  9.  9.  4.  4.  1.\n",
      "  2.  5.  6.  8.  7.  6.  8.  3. 10.  5.  5.  3. 10.  7.  9.  1.  3.  4.\n",
      "  4.  5.  3.  9.  5.  6.  9.  2.  1.  1.  4.  1.  9.  7.  7.  6.  3.  0.\n",
      "  9. 10.  1.  3.  6.  3.  6.  3.  2. 10.  3.  9. 10.  5.  1.  6.  4.  0.\n",
      "  9.  2.  9.  6.  3. 10.  3.  2.  0.  7.  8.  3.  8.  2.  7.  5.  7.  2.\n",
      "  4.  8.  7.  4.  2.  9.  8.  8.  6.  0.  8.  7.  4.  3.  3.  8.  4.  9.\n",
      "  4.  8.  8.  1.  8.  2.  1.  2.  6.  5.  5.  0.  7.  9.  1.  4.  1.  4.\n",
      "  1.  5.  2.  7. 10.  7.  2.  7.  4.  6.  2.  5.  9.  2.  9.  1.  2.  2.\n",
      "  6.  8.  2.  1.  3.  6.  6. 10.  1.  2.  7. 10.  5.  4.  0.  9.  6.  4.\n",
      " 10.  2.  2.  6. 10.  5.  9.  1.  7.  6.  7. 10.  3.  9.  6.  8.  4. 10.\n",
      "  5.  4.  7.  7.  4.  4.  7.  2.  7.  1.  2.  7.  4.  4.  8.  4.  7.  7.\n",
      "  2.  5.  7.  2. 10.  8.  9.  5.  8.  3.  6.  2. 10.  8.  7.  3.  7.  6.\n",
      "  5.  3.  1.  3.  2.  2.  5.  5.  1.  2.  9.  2.  7. 10.  7.  2.  1.  2.\n",
      "  2. 10.  2.  4.  7.  9.  0.  1. 10.  7.  7. 10.  7.  8.  4.  6.  3.  3.\n",
      " 10.  1.  3.  7. 10.  1.  3.  1.  4.  2.  3.  8.  4.  2.  3.  7.  8.  4.\n",
      "  2. 10.  9. 10. 10.  1. 10.  4.  4.  6.  7.  3.  1.  1.  3.  7.  7.  5.\n",
      "  2.  6.  6.  5.  8.  7.  1.  6.  8.  8.  3.  3. 10.  4. 10.  1.  3.  8.\n",
      "  8. 10.  6.  9.  9.  4.  7.  3.  8.  6. 10. 10.  4.  2.  3.  2.  7.  2.\n",
      "  2.  5.  9.  8.  9.  1.  7.  4. 10.  7. 10.  1.  4.  8.  3.  9.  6.  8.\n",
      "  4.  7. 10.  3.  7.  8.  9.  1.  1.  6.  6.  6.  3.  9.  1.  9.  9.  4.\n",
      "  6.  1.  7. 10.  6.  8.  8.  9.  2.  9. 10.  4.  7.  8.  3.  1.  2. 10.\n",
      "  1.  5.  8.  7.  4.  5.  8.  1.  3.  8.]\n"
     ]
    }
   ],
   "source": [
    "l=labels.cpu().numpy()[:1000]\n",
    "for i in range(1000):\n",
    "    if l[i]==0:\n",
    "        p[i]=10\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import manifold, datasets\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "pos = digits.data\n",
    "y = digits.target\n",
    "print(type(pos[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2, 2)\n",
      "Train Epoch: 0 \tLoss: 4.050958e-06\n",
      "Train Epoch: 0 \tLoss: 3.991271e-06\n",
      "Train Epoch: 0 \tLoss: 3.971874e-06\n",
      "Train Epoch: 0 \tLoss: 3.931830e-06\n",
      "Train Epoch: 0 \tLoss: 3.908011e-06\n",
      "Train Epoch: 0 \tLoss: 3.854957e-06\n",
      "Train Epoch: 0 \tLoss: 3.834711e-06\n",
      "Train Epoch: 0 \tLoss: 3.780564e-06\n",
      "Train Epoch: 0 \tLoss: 3.732472e-06\n",
      "Train Epoch: 0 \tLoss: 3.685790e-06\n",
      "Train Epoch: 0 \tLoss: 3.629099e-06\n",
      "Train Epoch: 0 \tLoss: 3.571091e-06\n",
      "Train Epoch: 0 \tLoss: 3.502069e-06\n",
      "Train Epoch: 0 \tLoss: 3.441946e-06\n",
      "Train Epoch: 0 \tLoss: 3.376016e-06\n",
      "Train Epoch: 0 \tLoss: 3.305479e-06\n",
      "Train Epoch: 0 \tLoss: 3.246400e-06\n",
      "Train Epoch: 0 \tLoss: 3.178814e-06\n",
      "Train Epoch: 0 \tLoss: 3.124155e-06\n",
      "Train Epoch: 0 \tLoss: 3.064881e-06\n",
      "Train Epoch: 0 \tLoss: 3.008895e-06\n",
      "Train Epoch: 0 \tLoss: 2.964761e-06\n",
      "Train Epoch: 0 \tLoss: 2.914757e-06\n",
      "Train Epoch: 0 \tLoss: 2.877794e-06\n",
      "Train Epoch: 0 \tLoss: 2.838146e-06\n",
      "Train Epoch: 0 \tLoss: 2.801914e-06\n",
      "Train Epoch: 0 \tLoss: 2.762549e-06\n",
      "Train Epoch: 0 \tLoss: 2.737990e-06\n",
      "Train Epoch: 0 \tLoss: 2.715401e-06\n",
      "Train Epoch: 0 \tLoss: 2.691176e-06\n",
      "Train Epoch: 0 \tLoss: 2.672445e-06\n",
      "Train Epoch: 0 \tLoss: 2.653025e-06\n",
      "Train Epoch: 0 \tLoss: 2.630020e-06\n",
      "Train Epoch: 0 \tLoss: 2.614541e-06\n",
      "Train Epoch: 0 \tLoss: 2.599679e-06\n",
      "Train Epoch: 0 \tLoss: 2.582607e-06\n",
      "Train Epoch: 0 \tLoss: 2.574774e-06\n",
      "Train Epoch: 0 \tLoss: 2.563025e-06\n",
      "Train Epoch: 0 \tLoss: 2.552160e-06\n",
      "Train Epoch: 0 \tLoss: 2.543689e-06\n",
      "Train Epoch: 0 \tLoss: 2.535256e-06\n",
      "Train Epoch: 0 \tLoss: 2.530936e-06\n",
      "Train Epoch: 0 \tLoss: 2.523093e-06\n",
      "Train Epoch: 0 \tLoss: 2.515523e-06\n",
      "Train Epoch: 0 \tLoss: 2.510971e-06\n",
      "Train Epoch: 0 \tLoss: 2.506561e-06\n",
      "Train Epoch: 0 \tLoss: 2.501681e-06\n",
      "Train Epoch: 0 \tLoss: 2.497573e-06\n",
      "Train Epoch: 0 \tLoss: 2.492171e-06\n",
      "Train Epoch: 0 \tLoss: 2.490166e-06\n",
      "Train Epoch: 0 \tLoss: 2.485717e-06\n",
      "Train Epoch: 0 \tLoss: 2.479792e-06\n",
      "Train Epoch: 0 \tLoss: 2.478892e-06\n",
      "Train Epoch: 0 \tLoss: 2.478113e-06\n",
      "Train Epoch: 0 \tLoss: 2.475747e-06\n",
      "Train Epoch: 0 \tLoss: 2.471345e-06\n",
      "Train Epoch: 0 \tLoss: 2.469276e-06\n",
      "Train Epoch: 0 \tLoss: 2.466217e-06\n",
      "Train Epoch: 0 \tLoss: 2.466045e-06\n",
      "Train Epoch: 0 \tLoss: 2.463358e-06\n",
      "Train Epoch: 0 \tLoss: 2.461482e-06\n",
      "Train Epoch: 0 \tLoss: 2.457449e-06\n",
      "Train Epoch: 0 \tLoss: 2.459180e-06\n",
      "Train Epoch: 0 \tLoss: 2.455153e-06\n",
      "Train Epoch: 0 \tLoss: 2.455095e-06\n",
      "Train Epoch: 0 \tLoss: 2.453614e-06\n",
      "Train Epoch: 0 \tLoss: 2.451079e-06\n",
      "Train Epoch: 0 \tLoss: 2.451135e-06\n",
      "Train Epoch: 0 \tLoss: 2.448733e-06\n",
      "Train Epoch: 0 \tLoss: 2.445439e-06\n",
      "Train Epoch: 0 \tLoss: 2.444822e-06\n",
      "Train Epoch: 0 \tLoss: 2.440835e-06\n",
      "Train Epoch: 0 \tLoss: 2.443309e-06\n",
      "Train Epoch: 0 \tLoss: 2.439101e-06\n",
      "Train Epoch: 0 \tLoss: 2.440976e-06\n",
      "Train Epoch: 0 \tLoss: 2.438172e-06\n",
      "Train Epoch: 0 \tLoss: 2.437718e-06\n",
      "Train Epoch: 0 \tLoss: 2.433892e-06\n",
      "Train Epoch: 0 \tLoss: 2.433556e-06\n",
      "Train Epoch: 0 \tLoss: 2.434138e-06\n",
      "Train Epoch: 0 \tLoss: 2.431082e-06\n",
      "Train Epoch: 0 \tLoss: 2.429304e-06\n",
      "Train Epoch: 0 \tLoss: 2.429139e-06\n",
      "Train Epoch: 0 \tLoss: 2.427588e-06\n",
      "Train Epoch: 0 \tLoss: 2.426975e-06\n",
      "Train Epoch: 0 \tLoss: 2.423384e-06\n",
      "Train Epoch: 0 \tLoss: 2.424174e-06\n",
      "Train Epoch: 0 \tLoss: 2.423631e-06\n",
      "Train Epoch: 0 \tLoss: 2.423227e-06\n",
      "Train Epoch: 0 \tLoss: 2.420534e-06\n",
      "Train Epoch: 0 \tLoss: 2.421596e-06\n",
      "Train Epoch: 0 \tLoss: 2.419206e-06\n",
      "Train Epoch: 0 \tLoss: 2.417748e-06\n",
      "Train Epoch: 0 \tLoss: 2.417425e-06\n",
      "Train Epoch: 0 \tLoss: 2.416193e-06\n",
      "Train Epoch: 0 \tLoss: 2.416466e-06\n",
      "Train Epoch: 0 \tLoss: 2.416778e-06\n",
      "Train Epoch: 0 \tLoss: 2.416177e-06\n",
      "Train Epoch: 0 \tLoss: 2.414089e-06\n",
      "Train Epoch: 0 \tLoss: 2.415680e-06\n",
      "Train Epoch: 0 \tLoss: 2.413715e-06\n",
      "Train Epoch: 0 \tLoss: 2.413745e-06\n",
      "Train Epoch: 0 \tLoss: 2.414410e-06\n",
      "Train Epoch: 0 \tLoss: 2.414485e-06\n",
      "Train Epoch: 0 \tLoss: 2.414577e-06\n",
      "Train Epoch: 0 \tLoss: 2.412386e-06\n",
      "Train Epoch: 0 \tLoss: 2.410690e-06\n",
      "Train Epoch: 0 \tLoss: 2.409848e-06\n",
      "Train Epoch: 0 \tLoss: 2.412216e-06\n",
      "Train Epoch: 0 \tLoss: 2.411201e-06\n",
      "Train Epoch: 0 \tLoss: 2.409385e-06\n",
      "Train Epoch: 0 \tLoss: 2.409674e-06\n",
      "Train Epoch: 0 \tLoss: 2.409800e-06\n",
      "Train Epoch: 0 \tLoss: 2.409698e-06\n",
      "Train Epoch: 0 \tLoss: 2.409377e-06\n",
      "Train Epoch: 0 \tLoss: 2.408556e-06\n",
      "Train Epoch: 0 \tLoss: 2.408040e-06\n",
      "Train Epoch: 0 \tLoss: 2.408569e-06\n",
      "Train Epoch: 0 \tLoss: 2.408240e-06\n",
      "Train Epoch: 0 \tLoss: 2.406947e-06\n",
      "Train Epoch: 0 \tLoss: 2.406300e-06\n",
      "Train Epoch: 0 \tLoss: 2.406221e-06\n",
      "Train Epoch: 0 \tLoss: 2.404920e-06\n",
      "Train Epoch: 0 \tLoss: 2.405855e-06\n",
      "Train Epoch: 0 \tLoss: 2.406133e-06\n",
      "Train Epoch: 0 \tLoss: 2.404200e-06\n",
      "Train Epoch: 0 \tLoss: 2.404873e-06\n",
      "Train Epoch: 0 \tLoss: 2.403676e-06\n",
      "Train Epoch: 0 \tLoss: 2.405137e-06\n",
      "Train Epoch: 0 \tLoss: 2.403149e-06\n",
      "Train Epoch: 0 \tLoss: 2.405178e-06\n",
      "Train Epoch: 0 \tLoss: 2.404505e-06\n",
      "Train Epoch: 0 \tLoss: 2.405104e-06\n",
      "Train Epoch: 0 \tLoss: 2.403728e-06\n",
      "Train Epoch: 0 \tLoss: 2.404800e-06\n",
      "Train Epoch: 0 \tLoss: 2.401252e-06\n",
      "Train Epoch: 0 \tLoss: 2.403101e-06\n",
      "Train Epoch: 0 \tLoss: 2.402463e-06\n",
      "Train Epoch: 0 \tLoss: 2.401484e-06\n",
      "Train Epoch: 0 \tLoss: 2.399349e-06\n",
      "Train Epoch: 0 \tLoss: 2.401128e-06\n",
      "Train Epoch: 0 \tLoss: 2.401813e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-fc0fd099093a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mwrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Visualize the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kez040/QUiz/wrapper.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     42\u001b[0m                     \u001b[0mdatas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatas\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kez040/QUiz/vtsne.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/kez040/QUiz/vtsne.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pij, i, j)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mxj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mxj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mqij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Compute KLD(pij || qij)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import manifold, datasets\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from wrapper import Wrapper\n",
    "# from tsne import TSNE\n",
    "from vtsne import VTSNE\n",
    "\n",
    "\n",
    "def preprocess(perplexity=30, metric='euclidean'):\n",
    "    \"\"\" Compute pairiwse probabilities for MNIST pixels.\n",
    "    \"\"\"\n",
    "    pos = b\n",
    "    y = l\n",
    "    n_points = pos.shape[0]\n",
    "    distances2 = pairwise_distances(pos, metric=metric, squared=True)\n",
    "    # This return a n x (n-1) prob array\n",
    "    pij = manifold.t_sne._joint_probabilities(distances2, perplexity, False)\n",
    "    # Convert to n x n prob array\n",
    "    pij = squareform(pij)\n",
    "    return n_points, pij, y\n",
    "\n",
    "\n",
    "draw_ellipse = True\n",
    "n_points, pij2d, y = preprocess()\n",
    "i, j = np.indices(pij2d.shape)\n",
    "i = i.ravel()\n",
    "j = j.ravel()\n",
    "pij = pij2d.ravel().astype('float32')\n",
    "# Remove self-indices\n",
    "idx = i != j\n",
    "i, j, pij = i[idx], j[idx], pij[idx]\n",
    "\n",
    "n_topics = 2\n",
    "n_dim = 2\n",
    "print(n_points, n_dim, n_topics)\n",
    "\n",
    "model = VTSNE(n_points, n_topics, n_dim)\n",
    "wrap = Wrapper(model, batchsize=4096, epochs=1)\n",
    "for itr in range(500):\n",
    "    wrap.fit(pij, i, j)\n",
    "\n",
    "    # Visualize the results\n",
    "    embed = model.logits.weight.cpu().data.numpy()\n",
    "    f = plt.figure()\n",
    "    if not draw_ellipse:\n",
    "        plt.scatter(embed[:, 0], embed[:, 1], c=y * 1.0 / y.max())\n",
    "        plt.axis('off')\n",
    "        plt.savefig('pic4/scatter_{:03d}.png'.format(itr), bbox_inches='tight')\n",
    "        plt.close(f)\n",
    "    else:\n",
    "        # Visualize with ellipses\n",
    "        var = np.sqrt(model.logits_lv.weight.clone().exp_().cpu().data.numpy())\n",
    "        ax = plt.gca()\n",
    "        for xy, (w, h), c in zip(embed, var, y):\n",
    "            e = Ellipse(xy=xy, width=w, height=h, ec=None, lw=0.0)\n",
    "            e.set_facecolor(plt.cm.Paired(c * 1.0 / y.max()))\n",
    "            e.set_alpha(0.5)\n",
    "            ax.add_artist(e)\n",
    "        ax.set_xlim(-9, 9)\n",
    "        ax.set_ylim(-9, 9)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('pic4/scatter_{:03d}.png'.format(itr), bbox_inches='tight')\n",
    "        plt.close(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2, 2]).to(device)\n",
    "model.fc = nn.Linear(128, 100).to(device) \n",
    "model.load_state_dict(torch.load('resnet_100.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Finetune on CIFAR10\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "# model.fc = nn.Linear(128, 100).to(device)  # 100 is an example.\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset_100 = torchvision.datasets.CIFAR100(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset_100 = torchvision.datasets.CIFAR100(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader_100 = torch.utils.data.DataLoader(dataset=train_dataset_100,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader_100 = torch.utils.data.DataLoader(dataset=test_dataset_100,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 0.8784\n",
      "Epoch [1/80], Step [200/500] Loss: 0.7954\n",
      "Epoch [1/80], Step [300/500] Loss: 0.7619\n",
      "Epoch [1/80], Step [400/500] Loss: 0.8145\n",
      "Epoch [1/80], Step [500/500] Loss: 0.8682\n",
      "Epoch [2/80], Step [100/500] Loss: 0.7558\n",
      "Epoch [2/80], Step [200/500] Loss: 0.7218\n",
      "Epoch [2/80], Step [300/500] Loss: 0.5851\n",
      "Epoch [2/80], Step [400/500] Loss: 0.7708\n",
      "Epoch [2/80], Step [500/500] Loss: 0.9946\n",
      "Epoch [3/80], Step [100/500] Loss: 1.1143\n",
      "Epoch [3/80], Step [200/500] Loss: 0.7900\n",
      "Epoch [3/80], Step [300/500] Loss: 0.7051\n",
      "Epoch [3/80], Step [400/500] Loss: 0.6518\n",
      "Epoch [3/80], Step [500/500] Loss: 1.0576\n",
      "Epoch [4/80], Step [100/500] Loss: 0.8085\n",
      "Epoch [4/80], Step [200/500] Loss: 0.8401\n",
      "Epoch [4/80], Step [300/500] Loss: 0.7448\n",
      "Epoch [4/80], Step [400/500] Loss: 1.0680\n",
      "Epoch [4/80], Step [500/500] Loss: 0.8961\n",
      "Epoch [5/80], Step [100/500] Loss: 0.6860\n",
      "Epoch [5/80], Step [200/500] Loss: 0.5728\n",
      "Epoch [5/80], Step [300/500] Loss: 0.8705\n",
      "Epoch [5/80], Step [400/500] Loss: 0.6110\n",
      "Epoch [5/80], Step [500/500] Loss: 0.7078\n",
      "Epoch [6/80], Step [100/500] Loss: 0.7827\n",
      "Epoch [6/80], Step [200/500] Loss: 0.8899\n",
      "Epoch [6/80], Step [300/500] Loss: 0.8160\n",
      "Epoch [6/80], Step [400/500] Loss: 0.5984\n",
      "Epoch [6/80], Step [500/500] Loss: 0.8505\n",
      "Epoch [7/80], Step [100/500] Loss: 0.6314\n",
      "Epoch [7/80], Step [200/500] Loss: 0.7400\n",
      "Epoch [7/80], Step [300/500] Loss: 0.9333\n",
      "Epoch [7/80], Step [400/500] Loss: 0.6938\n",
      "Epoch [7/80], Step [500/500] Loss: 0.6747\n",
      "Epoch [8/80], Step [100/500] Loss: 0.7800\n",
      "Epoch [8/80], Step [200/500] Loss: 0.7336\n",
      "Epoch [8/80], Step [300/500] Loss: 0.7075\n",
      "Epoch [8/80], Step [400/500] Loss: 0.9353\n",
      "Epoch [8/80], Step [500/500] Loss: 0.8150\n",
      "Epoch [9/80], Step [100/500] Loss: 0.8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1d19e3f17e6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# print(images)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate/10)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader_100)\n",
    "curr_lr = learning_rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_100):\n",
    "        # print(images)\n",
    "        # print(labels)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, features= model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet_100.ckpt')\n",
    "torch.save(model, 'model_100.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[3.84278941 3.68966603 2.30542397 ... 4.09157228 0.75072026 4.64199352]\n",
      "[72. 33. 55. ... 51. 88. 70.]\n",
      "Accuracy of the model on the test images: 61 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model  \n",
    "pred=[]\n",
    "total_feature=[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i=0\n",
    "    for images, labels in test_loader_100:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs,feature = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # print(feature.cpu().detach().numpy().shape)\n",
    "        pred=np.concatenate((pred,predicted.cpu().numpy()))\n",
    "        total_feature=np.append(total_feature,feature.cpu().detach().numpy())\n",
    "    print(total)\n",
    "    print(total_feature)\n",
    "    print(pred)\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
